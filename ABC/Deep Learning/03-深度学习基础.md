# 3. 深度学习基础

> [3.1 线性回归 - Dive-into-DL-PyTorch (tangshusen.me)](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.1_linear-regression)

## 3.1 线性回归

### 3.1.1 线性回归的基本要素

#### 3.1.1.1 模型定义

模型（model）

模型的参数（parameter）

- 权重（weight）

- 偏差（bias）
- etc

#### 3.1.1.2 训练模型

##### (1) 训练数据

训练数据集（training data set）或训练集（training set）

样本（sample）

标签（label）

特征（feature）

##### (2) 损失函数

损失函数（loss function）

- 平方损失（square loss）。常数1/2使求导后的常数系数为1。
  $$
  \mathcal{l}^{(i)}(w_1,w_2,b) = \frac{1}{2}(\hat{y}^{(i)}-y^{(i)})^2
  $$
  

在模型训练中，我们希望找出一组模型参数，来使训练样本平均损失最小。

##### (3) 优化算法

解析解（analytical solution）

数值解（numerical solution）

大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。

小批量随机梯度下降（mini-batch stochastic gradient descent）：

- 先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch）$\mathcal{B}$，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。

超参数（hyperparameter）：

- 批量大小（batch size）$\vert\mathcal{B}\vert$
- 学习率（learning rate）$\eta$

- etc

我们通常所说的“调参”指的正是调节超参数，例如通过反复试错来找到超参数合适的值。在少数情况下，超参数也可以通过模型训练学出。

#### 3.1.1.3 模型预测

模型训练完成后，得到模型参数在优化算法停止时的值，用学出的模型来估算训练数据集以外结果，这里的估算也叫作模型预测、模型推断或模型测试。

### 3.1.2 线性回归的表示方法

#### 3.1.2.1 神经网络图

在深度学习中，我们可以使用神经网络图直观地表现模型结构。图3.1使用神经网络图表示本节中介绍的线性回归模型。神经网络图隐去了模型参数权重和偏差。

![图3.1 线性回归是一个单层神经网络](assets/3.1_linreg.svg)

在图3.1所示的神经网络中，输入层的输入个数为2。输入个数也叫**特征数**或**特征向量维度**。

由于输入层并不涉及计算，按照惯例，图3.1所示的神经网络的层数为1。

输出层中负责计算*o*的单元又叫神经元。

在线性回归中，*o* 的计算依赖于 x1 和 x2 。也就是说，输出层中的神经元和输入层中各个输入完全连接。因此，这里的输出层又叫**全连接层（fully-connected layer）**或**稠密层（dense layer）**。

#### 3.1.2.2 矢量计算表达式

当样本数为 n，特征数为 d 时，线性回归的矢量计算表达式为
$$
\boldsymbol{\hat{y}}=\boldsymbol{X}\boldsymbol{\omega}+b
$$
其中模型输出 $\boldsymbol{\hat{y}}\in\R^{n\times1}$，批量数据样本特征 $\boldsymbol{X}\in\R^{n\times d}$，权重 $\boldsymbol\omega\in\R^{d\times1}$，偏差$b\in\R$。相应地，批量数据样本标签 $\boldsymbol{y}\in\R^{n\times1}$。

设模型参数 $\theta$，我们可以重写损失函数为
$$
\mathcal{l}(\boldsymbol{\theta})=\frac{1}{2n}(\hat{\boldsymbol{y}}-\boldsymbol{y})^\text{T}(\hat{\boldsymbol{y}}-\boldsymbol{y})
$$
小批量随机梯度下降的迭代步骤将相应地改写为
$$
\boldsymbol{\theta} \gets \boldsymbol{\theta}-\frac{\eta}{\vert\mathcal{B}\vert}\Sigma_{i\in\mathcal{B}}\nabla_\boldsymbol{\theta}\mathcal{l}^{(i)}(\boldsymbol{\theta})
$$


