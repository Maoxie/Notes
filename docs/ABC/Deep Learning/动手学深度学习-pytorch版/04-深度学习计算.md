# 4. 深度学习计算

> [4.1 模型构造 - Dive-into-DL-PyTorch (tangshusen.me)](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter04_DL_computation/4.1_model-construction)

## 4.1 模型构造

### 4.1.1 继承`Module`类来构造模型

`Module`类是`nn`模块里提供的一个模型构造类，是所有神经网络模块的基类，我们可以继承它来定义我们想要的模型。

重载`Module`类的`__init__`函数和`forward`函数。它们分别用于创建模型参数和定义前向计算。前向计算也即正向传播。

无须定义反向传播函数。系统将通过自动求梯度而自动生成反向传播所需的`backward`函数。

```python
import torch
from torch import nn

class MLP(nn.Module):
    # 声明带有模型参数的层，这里声明了两个全连接层
    def __init__(self, **kwargs):
        # 调用MLP父类Module的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数
        # 参数，如“模型参数的访问、初始化和共享”一节将介绍的模型参数params
        super(MLP, self).__init__(**kwargs)
        self.hidden = nn.Linear(784, 256) # 隐藏层
        self.act = nn.ReLU()
        self.output = nn.Linear(256, 10)  # 输出层


    # 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出
    def forward(self, x):
        a = self.act(self.hidden(x))
        return self.output(a)
```

实例化得到模型变量，传入数据做一次前向计算。`Module`类的`__call__`函数将调用`MLP`类定义的`forward`函数来完成前向计算。

```python
X = torch.rand(2, 784)
net = MLP()
net(X)
```

`Module`类没有被命名为Layer或Model之类的名字，因为它是一个可供自由组建的部件。它的子类既可以是一个层（如PyTorch提供的`Linear`类），又可以是一个模型（如这里定义的`MLP`类），或者是模型的一个部分。

### 4.1.2 `Module`的子类

PyTorch还实现了继承自`Module`的可以方便构建模型的类: 如`Sequential`、`ModuleList`和`ModuleDict`等等。

#### 4.1.2.1 `Sequential`类

当模型的前向计算为简单串联各个层的计算时，`Sequential`类可以通过更加简单的方式定义模型。

它可以接收一个子模块的有序字典（OrderedDict）或者一系列子模块作为参数来逐一添加`Module`的实例，而模型的前向计算就是将这些实例按添加的顺序逐一计算。

工作原理：

```python
class MySequential(nn.Module):
    from collections import OrderedDict
    def __init__(self, *args):
        super(MySequential, self).__init__()
        if len(args) == 1 and isinstance(args[0], OrderedDict): # 如果传入的是一个OrderedDict
            for key, module in args[0].items():
                self.add_module(key, module)  # add_module方法会将module添加进self._modules(一个OrderedDict)
        else:  # 传入的是一些Module
            for idx, module in enumerate(args):
                self.add_module(str(idx), module)
    def forward(self, input):
        # self._modules返回一个 OrderedDict，保证会按照成员添加时的顺序遍历成员
        for module in self._modules.values():
            input = module(input)
        return input
```

#### 4.1.2.2 `ModuleList`类

`ModuleList`接收一个子模块的列表作为输入，然后也可以类似List那样进行append和extend操作:

```python
net = nn.ModuleList([nn.Linear(784, 256), nn.ReLU()])
net.append(nn.Linear(256, 10)) # # 类似List的append操作
print(net[-1])  # 类似List的索引访问
print(net)
# net(torch.zeros(1, 784)) # 会报NotImplementedError
```

`Sequential`和`ModuleList`都可以进行列表化构造网络，二者的区别是：

- `ModuleList`仅仅是一个储存各种模块的列表，这些模块之间没有联系也没有顺序（所以不用保证相邻层的输入输出维度匹配），而且没有实现`forward`功能需要自己实现，所以上面执行`net(torch.zeros(1, 784))`会报`NotImplementedError`；
- 而`Sequential`内的模块需要按照顺序排列，要保证相邻层的输入输出大小相匹配，内部`forward`功能已经实现。

`ModuleList`的出现只是让网络定义前向传播时更加灵活，见下面官网的例子。

```python
class MyModule(nn.Module):
    def __init__(self):
        super(MyModule, self).__init__()
        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])

    def forward(self, x):
        # ModuleList can act as an iterable, or be indexed using ints
        for i, l in enumerate(self.linears):
            x = self.linears[i // 2](x) + l(x)
        return x
```

另外，`ModuleList`不同于一般的Python的`list`，加入到`ModuleList`里面的所有模块的参数会被自动添加到整个网络中。

#### 4.1.2.3 `ModuleDict`类

`ModuleDict`接收一个子模块的字典作为输入, 然后也可以类似字典那样进行添加访问操作:

```python
net = nn.ModuleDict({
    'linear': nn.Linear(784, 256),
    'act': nn.ReLU(),
})
net['output'] = nn.Linear(256, 10) # 添加
print(net['linear']) # 访问
print(net.output)
print(net)
# net(torch.zeros(1, 784)) # 会报NotImplementedError
```

和`ModuleList`一样，`ModuleDict`实例仅仅是存放了一些模块的字典，并没有定义`forward`函数需要自己定义。同样，`ModuleDict`也与Python的`Dict`有所不同，`ModuleDict`里的所有模块的参数会被自动添加到整个网络中。

### 4.1.3 构造复杂的模型

虽然上面介绍的这些类可以使模型构造更加简单，且不需要定义`forward`函数，但直接继承`Module`类可以极大地拓展模型构造的灵活性。

> [4.1.3 构造复杂的模型](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter04_DL_computation/4.1_model-construction?id=_413-构造复杂的模型)

## 4.2 模型参数的访问、初始化和共享

本节将深入讲解如何访问和初始化模型参数，以及如何在多个层之间共享同一份模型参数。

### 4.2.1 访问模型参数

对于`Sequential`实例中含模型参数的层，我们可以通过`Module`类的`parameters()`或者`named_parameters`方法来访问所有参数（以迭代器的形式返回），后者除了返回参数`Tensor`外还会返回其名字。

```python
for name, param in net.named_parameters():
    print(name, param.size())
# (返回的名字自动加上了层数的索引作为前缀)output:
# 0.weight torch.Size([3, 4])
# 0.bias torch.Size([3])
# 2.weight torch.Size([1, 3])
# 2.bias torch.Size([1])
```

对于使用`Sequential`类构造的神经网络，我们可以通过方括号`[]`来访问网络的任一层。

返回的`param`的类型为`torch.nn.parameter.Parameter`，其实这是`Tensor`的子类，和`Tensor`不同的是如果一个`Tensor`是`Parameter`，那么它会自动被添加到模型的参数列表里。

### 4.2.2 初始化模型参数

Pytorch的`torch.nn.init`模块里提供了多种预设的初始化方法。

### 4.2.3 自定义初始化方法

有时候我们需要的初始化方法并没有在`init`模块中提供。这时，可以实现一个初始化方法，从而能够像使用其他初始化方法那样使用它。

先来看看PyTorch是怎么实现这些初始化方法的，例如`torch.nn.init.normal_`：

```python
def normal_(tensor, mean=0, std=1):
    with torch.no_grad():
        return tensor.normal_(mean, std)
```

这就是一个inplace改变`Tensor`值的函数，而且这个过程是不记录梯度的。类似的我们可以实现一个自定义的初始化方法。

此外，我们还可以通过改变参数的`data`来改写模型参数值同时不会影响梯度：

```python
for name, param in net.named_parameters():
    if 'bias' in name:
        param.data += 1
```

### 4.2.4 共享模型参数

在有些情况下，我们希望在多个层之间共享模型参数。

方法1：`Module`类的`forward`函数里多次调用同一个层。

方法2：如果我们传入`Sequential`的模块是同一个`Module`实例的话参数也是共享的

因为模型参数里包含了梯度，所以在反向传播计算时，这些共享的参数的梯度是累加的。

## 4.3 模型参数的延后初始化

由于使用Gluon创建的全连接层的时候不需要指定输入个数。所以当调用`initialize`函数时，由于隐藏层输入个数依然未知，系统也无法得知该层权重参数的形状。只有在当形状已知的输入`X`传进网络做前向计算`net(X)`时，系统才推断出该层的权重参数形状为多少，此时才进行真正的初始化操作。但是使用PyTorch在定义模型的时候就要指定输入的形状，所以也就不存在这个问题了，所以本节略。

## 4.4 自定义层

深度学习的一个魅力在于神经网络中各式各样的层，例如全连接层和后面章节中将要介绍的卷积层、池化层与循环层。虽然PyTorch提供了大量常用的层，但有时候我们依然希望自定义层。本节将介绍如何使用`Module`来自定义层，从而可以被重复调用。

### 4.4.1 不含模型参数的自定义层

定义一个不含模型参数的自定义层。这和4.1节（模型构造）中介绍的使用`Module`类构造模型类似。

下面的`CenteredLayer`类通过继承`Module`类自定义了一个将输入减掉均值后输出的层，并将层的计算定义在了`forward`函数里。这个层里不含模型参数。

```python
import torch
from torch import nn

class CenteredLayer(nn.Module):
    def __init__(self, **kwargs):
        super(CenteredLayer, self).__init__(**kwargs)
    def forward(self, x):
        return x - x.mean()
```

### 4.4.2 含模型参数的自定义层

自定义含模型参数的自定义层。其中的模型参数可以通过训练学出。

如果一个`Tensor`是`Parameter`，那么它会自动被添加到模型的参数列表里。在自定义含模型参数的层时，我们应该将参数定义成`Parameter`，除了像4.2.1节那样直接定义成`Parameter`类外，还可以使用`ParameterList`和`ParameterDict`分别定义参数的列表和字典。

- `ParameterList`接收一个`Parameter`实例的列表作为输入然后得到一个参数列表，使用的时候可以用索引来访问某个参数，另外也可以使用`append`和`extend`在列表后面新增参数。

- `ParameterDict`接收一个`Parameter`实例的字典作为输入然后得到一个参数字典，然后可以按照字典的规则使用了。例如使用`update()`新增参数，使用`keys()`返回所有键值，使用`items()`返回所有键值对等等，可参考[官方文档](https://pytorch.org/docs/stable/nn.html#parameterdict)。

## 4.5 读取和存储

在实际中，我们有时需要把训练好的模型部署到很多不同的设备。在这种情况下，我们可以把内存中训练好的模型参数存储在硬盘上供后续读取使用。

### 4.5.1 读写`Tensor`

我们可以直接使用`save`函数和`load`函数分别存储和读取`Tensor`。

- `save`使用Python的pickle实用程序将对象进行序列化，然后将序列化的对象保存到disk，使用`save`可以保存各种对象,包括模型、张量和字典等。

- `load`使用pickle unpickle工具将pickle的对象文件反序列化为内存。

```python
import torch
from torch import nn

x = torch.ones(3)
torch.save(x, 'x.pt')

x2 = torch.load('x.pt')
```

还可以存储一个`Tensor`列表并读回内存。

```python
y = torch.zeros(4)
torch.save([x, y], 'xy.pt')
xy_list = torch.load('xy.pt')
```

存储并读取一个从字符串映射到`Tensor`的字典。

```python
torch.save({'x': x, 'y': y}, 'xy_dict.pt')
xy = torch.load('xy_dict.pt')
```

### 4.5.2 读写模型

#### 4.5.2.1 `state_dict`

在PyTorch中，`Module`的可学习参数(即权重和偏差)，模块模型包含在参数中(通过`model.parameters()`访问)。`state_dict`是一个从参数名称映射到参数`Tesnor`的字典对象。

```python
net = MLP()
net.state_dict()
# output:
# OrderedDict([('hidden.weight', tensor([[ 0.2448,  0.1856, -0.5678],
#                       [ 0.2030, -0.2073, -0.0104]])),
#              ('hidden.bias', tensor([-0.3117, -0.4232])),
#              ('output.weight', tensor([[-0.4556,  0.4084]])),
#              ('output.bias', tensor([-0.3573]))])
```

只有具有可学习参数的层(卷积层、线性层等)才有`state_dict`中的条目。优化器(`optim`)也有一个`state_dict`，其中包含关于优化器状态以及所使用的超参数的信息。

#### 4.5.2.2 保存和加载模型

PyTorch中保存和加载训练模型有两种常见的方法:

1. 仅保存和加载模型参数(`state_dict`)；
2. 保存和加载整个模型。

**1. 仅保存和加载`state_dict`(推荐方式)**

```python
# 保存
torch.save(model.state_dict(), PATH) # 推荐的文件后缀名是pt或pth
# 加载
model = TheModelClass(*args, **kwargs)
model.load_state_dict(torch.load(PATH))
```

**2. 保存和加载整个模型**

```python
# 保存
torch.save(model, PATH)
# 加载
model = torch.load(PATH)
```

此外，还有一些其他使用场景，例如GPU与CPU之间的模型保存与读取、使用多块GPU的模型的存储等等，使用的时候可以参考[官方文档](https://pytorch.org/tutorials/beginner/saving_loading_models.html)。

## 4.6 GPU计算

在本节中，我们将介绍如何使用单块NVIDIA GPU来计算。所以需要确保已经安装好了PyTorch GPU版本。准备工作都完成后，下面就可以通过`nvidia-smi`命令来查看显卡信息了。

### 4.6.1 计算设备

PyTorch可以指定用来存储和计算的设备，如使用内存的CPU或者使用显存的GPU。默认情况下，PyTorch会将数据创建在内存，然后利用CPU来计算。

用`torch.cuda.is_available()`查看GPU是否可用:

```python
import torch
from torch import nn

torch.cuda.is_available() # 输出 True
```

查看GPU数量：

```python
torch.cuda.device_count() # 输出 1
```

查看当前GPU索引号，索引号从0开始：

```python
torch.cuda.current_device() # 输出 0
```

根据索引号查看GPU名字:

```python
torch.cuda.get_device_name(0) # 输出 'GeForce GTX 1050'
```

### 4.6.2 `Tensor`的GPU计算

默认情况下，`Tensor`会被存在内存上。因此，之前我们每次打印`Tensor`的时候看不到GPU相关标识。

```python
x = tensor([1, 2, 3], device='cuda:0')
x
# output:
# tensor([1, 2, 3])
```

使用`.cuda()`可以将CPU上的`Tensor`转换（复制）到GPU上。如果有多块GPU，我们用`.cuda(i)`来表示第 i 块GPU及相应的显存（i 从0开始）且`cuda(0)`和`cuda()`等价。

```python
x = x.cuda(0)
x
# output:
# tensor([1, 2, 3], device='cuda:0')
```

我们可以通过`Tensor`的`device`属性来查看该`Tensor`所在的设备。

```python
x.device
# output:
# device(type='cuda', index=0)
```

我们可以直接在创建的时候就指定设备。

```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

x = torch.tensor([1, 2, 3], device=device)
# or
x = torch.tensor([1, 2, 3]).to(device)
```

如果对在GPU上的数据进行运算，那么结果还是存放在GPU上。

需要注意的是，存储在不同位置中的数据是不可以直接进行计算的。即存放在CPU上的数据不可以直接与存放在GPU上的数据进行运算，位于不同GPU上的数据也是不能直接进行计算的。

### 4.6.3 模型的GPU计算

同`Tensor`类似，PyTorch模型也可以通过`.cuda`转换到GPU上。我们可以通过检查模型的参数的`device`属性来查看存放模型的设备。

```python
net = nn.Linear(3, 1)
list(net.parameters())[0].device
# output:
# device(type='cpu')
```

将其转换到GPU上:

```python
net.cuda()
list(net.parameters())[0].device
# output:
# device(type='cuda', index=0)
```

同样的，我么需要保证模型输入的`Tensor`和模型都在同一设备上，否则会报错。

